{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Pymoo plugin for <code>ropt</code>","text":"<p>The <code>ropt-dakota</code> package extends the <code>ropt</code> module by providing a plugin that integrates optimization algorithms from the <code>pymoo</code> toolkit.  <code>ropt</code> itself is a robust optimization framework designed for both continuous and discrete optimization workflows and is extensible through its plugin architecture. Installing <code>ropt-pymoo</code> makes these <code>pymoo</code> algorithms directly available within <code>ropt</code>.</p>"},{"location":"#usage","title":"Usage","text":"<p>An optimization by ropt using the plugin works mostly as any other optimization run. However, there are a few things to consider:</p> <ol> <li>Gradients are not used, as <code>pymoo</code> does not seem to support passing    user-defined gradients. Hence, any specifications relating to gradient    calulcations in ropt are ignored.</li> <li>Some standard optimization parameters that can be specified in the    optimization section are ignored, specifically:<ul> <li><code>max_iterations</code></li> <li><code>tolerance</code></li> </ul> </li> <li>The initial values of the variables are ignored, since <code>pymoo</code> generally does    not use them. In ropt you still need to specify them, since the size of the    vector determines the number of variables. Setting it to a vector of zero    values is fine.</li> <li>Linear and non-linear constraints are both supported. Linear constraints are    not supported directly, but are internally converted to non-linear    constraints.</li> <li>The algorithm and its options are specified using a syntax closely following    the <code>pymoo</code> manual. For instance, rather than just giving an algorithm name,    you have to specify the full qualified name of the corresponding object as    found in the <code>pymoo.algorithms</code> module. For instance to specify the <code>GA</code>    algorithm, use: <code>soo.nonconvex.ga.GA</code>.</li> <li>The algorithms itself are entirely configured via the <code>options</code> field in the    optimization section of the ropt configuration object. Also in this case, the    syntax follows the <code>pymoo</code> manual. See the section below for more    information.</li> </ol>"},{"location":"#configuring-an-algorithm","title":"Configuring an algorithm.","text":"<p>Configuration of any of the <code>pymoo</code> algorithms is done via the options field in the ropt configuration object. For instance, consider this example for starting a <code>GA</code> optimization from the <code>pymoo</code> manual, with a penalty constraint added:</p> <pre><code>from pymoo.algorithms.soo.nonconvex.ga import GA\nfrom pymoo.operators.crossover.sbx import SBX\nfrom pymoo.operators.mutation.pm import PM\nfrom pymoo.operators.repair.rounding import RoundingRepair\nfrom pymoo.operators.sampling.rnd import IntegerRandomSampling\nfrom pymoo.optimize import minimize\nfrom pymoo.constraints.as_penalty import ConstraintsAsPenalty\n\nmethod = GA(\n    pop_size=20,\n    sampling=IntegerRandomSampling(),\n    crossover=SBX(prob=1.0, eta=3.0, vtype=float, repair=RoundingRepair()),\n    mutation=PM(prob=1.0, eta=3.0, vtype=float, repair=RoundingRepair()),\n    eliminate_duplicates=True,\n)\n\nres = minimize(ConstraintsAsPenalty(\n    problem, penalty=100.0),\n    method,\n    termination=('n_gen', 40),\n    seed=1234,\n)\n</code></pre> <p>To run the equivalent optimization, we need to specify the method and the termination in the options field. We also  need to specify the constraints object, and a seed. To do this the different objects are specified with their parameters in a nested dictionary that will be parsed into equivalent code. For this example we need to pass a nested dict, for clarity displayed as yml here: <pre><code>parameters:  # The parameters of the GA object:\n    pop_size: 20\n    sampling:  # The sampling parameter is an object, specify its full path in pymoo:\n    object: operators.sampling.rnd.IntegerRandomSampling\n    crossover:  # Also an object:\n    object: operators.crossover.sbx.SBX\n    parameters:  # Specify the parameters passed to the crossover ojbect:\n        prob: 1.0\n        eta: 3.0\n        vtype: float\n        repair:  # A repair object, passed to the crossover object:\n        object: operators.repair.rounding.RoundingRepair\n    mutation:  # An object:\n    object: operators.mutation.pm.PM\n    parameters:  # And its parameters:\n        prob: 1.0\n        eta: 3.0\n        vtype: float\n        repair:  # A repair object, passed to the mutation object:\n        object: operators.repair.rounding.RoundingRepair\n    eliminate_duplicates: True\ntermination:  # Specification of the termination object:\n    name: max_gen.MaximumGenerationTermination\n    parameters:\n        n_max_gen: 10\n# Alternative specification for the termination, following pymoo practice:\n# \"termination\": (\"n_iter\", 30)\nconstraints:  # Specification of the constraint object:\n    name: as_penalty.ConstraintsAsPenalty\n    parameters:\n        penalty: 100.0\nseed: 1234  # The seed that is passed to the minimize function:\n</code></pre></p>"},{"location":"#reference","title":"Reference","text":""},{"location":"#ropt_pymoo.pymoo.PyMooOptimizer","title":"ropt_pymoo.pymoo.PyMooOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Pymoo optimization backend for ropt.</p> <p>This class provides an interface to several optimization algorithms from <code>pymoo</code>, enabling their use within <code>ropt</code>.</p> <p>To select an optimizer, set the <code>method</code> field within the <code>optimizer</code> section of the <code>EnOptConfig</code> configuration object to the desired algorithm's name. Most methods support the general options defined in the <code>EnOptConfig</code> object. For algorithm-specific options, use the <code>options</code> dictionary within the <code>optimizer</code> section.</p>"}]}